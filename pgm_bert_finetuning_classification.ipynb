{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\EsdrasDaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\EsdrasDaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\EsdrasDaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\EsdrasDaniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 - Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(texto: str, formatacao: str = 'NFD', remover_especiais=True):\n",
    "    \"\"\"\n",
    "    Função para normalizar texto, removendo acentos e caracteres especiais.\n",
    "    Parâmetros:\n",
    "    texto: str - texto a ser normalizado\n",
    "    formatacao: str - formatação a ser utilizada para normalização. Padrão: NFD\n",
    "    Retorna:\n",
    "    str - texto normalizado\n",
    "    Exemplo:\n",
    "    string_text = 'áéíóúçãõ ção -- __ ¬ ²³£¢¬9)( \" ///°°?<>) jisadsa !// 1234 /*-+,&%$#@!'\n",
    "    for form in ['NFC', 'NFKC', 'NFD', 'NFKD']:\n",
    "        print(normalizar_texto(string_text, form))\n",
    "    \"\"\"\n",
    "\n",
    "    # check valid formatacao\n",
    "    if formatacao not in ['NFC', 'NFKC', 'NFD', 'NFKD']:\n",
    "        raise ValueError('formatacao must be one of: NFC, NFKC, NFD, NFKD')\n",
    "\n",
    "    if texto:\n",
    "        # convert texto to str if not\n",
    "        if type(texto) is not str:\n",
    "            texto = str(texto)\n",
    "        nfkd = unicodedata.normalize(formatacao, texto)  # NFKD\n",
    "        palavra_sem_acento = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "        if remover_especiais is True:\n",
    "            palavra_sem_acento = re.sub('[^a-zA-Z0-9 \\\\\\]', '', palavra_sem_acento)\n",
    "\n",
    "        return palavra_sem_acento.replace('  ', ' ')\n",
    "\n",
    "    return texto\n",
    "\n",
    "def preprocess_text_safe(text, with_space=True):\n",
    "    \"\"\"\n",
    "    Realiza o pré-processamento de um texto, ignorando erros.\n",
    "\n",
    "    Parâmetros:\n",
    "        - text (str): Texto judicial a ser processado.\n",
    "\n",
    "    Retorna:\n",
    "        - Texto limpo e lematizado (str), ou uma string vazia se ocorrer um erro.\n",
    "    \"\"\"\n",
    "\n",
    "    erros = []\n",
    "\n",
    "    try:\n",
    "\n",
    "        strange_words = [\n",
    "            'assim',\n",
    "            'parte',\n",
    "            'rozana',\n",
    "            'ato',\n",
    "            'grande',\n",
    "            'juiz djanirito souza moura',\n",
    "            'cidade',\n",
    "            'estado rio grande norte',\n",
    "            'rio norte',\n",
    "            'assinado',\n",
    "            '36738671',\n",
    "            'normal',\n",
    "            'cod',\n",
    "            'juiz',\n",
    "            'ptbr',\n",
    "            'lei',\n",
    "            'praca sete setembro',\n",
    "            'auto',\n",
    "            'forma',\n",
    "            'secretaria',\n",
    "            'cidade alta',\n",
    "            'data',\n",
    "            'tributaria natal',\n",
    "            'nao',\n",
    "            '5902530',\n",
    "            '59025275',\n",
    "            'alta',\n",
    "            'email',\n",
    "            'rn',\n",
    "            'publico',\n",
    "            'valor',\n",
    "            'intimese',\n",
    "            'setembro',\n",
    "            'norte',\n",
    "            'whatsapp',\n",
    "            'digitalmente',\n",
    "            '2024',\n",
    "            'natalrn',\n",
    "            'judiciario',\n",
    "            'secuniefttj',\n",
    "            '59025300',\n",
    "            'comarca',\n",
    "            'obrig',\n",
    "            'termos',\n",
    "            'apos',\n",
    "            '1141906',\n",
    "            'estado',\n",
    "            'processo',\n",
    "            'rel',\n",
    "            'forum fazendario',\n",
    "            'forum',\n",
    "            'fazendario',\n",
    "            'norte',\n",
    "            'telefone',\n",
    "            'vara',\n",
    "            'documento',\n",
    "            'n1141906',\n",
    "            'caso',\n",
    "            'publica',\n",
    "            'poder judiciario',\n",
    "            'poder',\n",
    "            'silva',\n",
    "            'xnone',\n",
    "            'prazo',\n",
    "            'justica',\n",
    "            'juiz djanirito souza mouro',\n",
    "            'igo',\n",
    "            'ate',\n",
    "            'data registrada sistema',\n",
    "            'sobre',\n",
    "            'false',\n",
    "            'sistema',\n",
    "            'codigo',\n",
    "            'dia',\n",
    "            'direito',\n",
    "            'cpc',\n",
    "            'veft',\n",
    "            'jusbr',\n",
    "            'desde',\n",
    "            'civil',\n",
    "            'vara execucao fiscal',\n",
    "            'juiza',\n",
    "            'valores',\n",
    "            'art',\n",
    "            'cep',\n",
    "            'conforme',\n",
    "            'natal',\n",
    "            'natalpraca',\n",
    "            'forma lei',\n",
    "            'tributaria',\n",
    "            'rio',\n",
    "            'praca alto',\n",
    "            'acao',\n",
    "            'municipio',\n",
    "            'sete',\n",
    "            'voltem',\n",
    "            'conclusos',\n",
    "            'publiquese',\n",
    "            'cumprase',\n",
    "            'artigo',\n",
    "            'bem',\n",
    "            'presente',\n",
    "            'devera',\n",
    "            'sendo',\n",
    "            'ano',\n",
    "            'inciso',\n",
    "            'maria',\n",
    "            'disposto',\n",
    "            'ainda',\n",
    "            'federal',\n",
    "            'turma',\n",
    "            'feito',\n",
    "            'meio',\n",
    "            'intimemse',\n",
    "            'partir',\n",
    "            'sob',\n",
    "            'dje',\n",
    "            'junho',\n",
    "            'intimo',\n",
    "            'julho',\n",
    "            'manifestarse',\n",
    "            'chefe',\n",
    "            '59025300contato',\n",
    "            '203'\n",
    "        ]\n",
    "\n",
    "        # Inicializar lematizador e stopwords\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "        text = normalizar_texto(text, 'NFD', remover_especiais=True)\n",
    "\n",
    "        # Tokenizar texto\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remover stopwords e lematizar tokens\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2\n",
    "        ]\n",
    "\n",
    "        # Reconstruir o texto processado\n",
    "        processed_text = \" \".join(tokens)\n",
    "\n",
    "        # Remover strange_words com Regex\n",
    "        pattern_re = \"|\".join(strange_words)\n",
    "\n",
    "        processed_text = re.sub(\n",
    "           f\"\\\\b({pattern_re})\\\\b\", \"\", processed_text, flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        # remover espacos duplos\n",
    "        processed_text = \" \".join(re.split(r\"\\s+\", processed_text))\n",
    "        if with_space is False:\n",
    "            processed_text = processed_text.replace(\" \", \"\")\n",
    "\n",
    "        return processed_text\n",
    "\n",
    "    except Exception as e:\n",
    "        erro = str(e)\n",
    "        if erro not in erros:\n",
    "            erros.append(erro)\n",
    "            print(f\"Erro ao processar texto: {erro}\")\n",
    "            return \"\"  # Retorna texto vazio em caso de erro\n",
    "        \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def separar_e_dividir_dados(df: pd.DataFrame, coluna_classe: str, n_samples_per_class: int = 500, train_frac:float = 0.7, test_frac: float = 0.15, val_frac: float = 0.15):\n",
    "    \"\"\"\n",
    "    Separa os dados de um DataFrame com classes desbalanceadas e divide em treinamento, validação e teste.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (pd.DataFrame): O dataframe contendo os dados.\n",
    "        coluna_classe (str): O nome da coluna que contém as classes.\n",
    "        min_ocorrencias (int): Número mínimo de ocorrências para uma classe ser considerada balanceada.\n",
    "        \n",
    "    Retorna:\n",
    "        tuple: DataFrames de treinamento, validação e teste.\n",
    "    \"\"\"\n",
    "    soma = sum([train_frac, test_frac, val_frac])\n",
    "    if abs(soma - 1) > 1e-6:\n",
    "        raise ValueError(f\"A soma das porcentagem de split dos dados é {soma}, que não é aproximadamente igual a 1. Tolerância = 1e-6\")\n",
    "\n",
    "    train_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    \n",
    "    # Itera sobre cada classe única\n",
    "    for classe, grupo in df.groupby(coluna_classe):\n",
    "        if len(grupo) <= n_samples_per_class:\n",
    "            # Utiliza todos os dados\n",
    "            grupo = grupo.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "            \n",
    "            '''train_size = int(train_frac * len(grupo))\n",
    "            test_size = int(test_frac * len(grupo))\n",
    "            \n",
    "            train_df = pd.concat(train_df, grupo[:train_size])\n",
    "            test_df = pd.concat(test_df, grupo[train_size:train_size+test_size])\n",
    "            val_df = pd.concat(val_df, grupo[train_size+test_size:])'''\n",
    "            \n",
    "        else:\n",
    "            # Limita ao número de amostras por classe\n",
    "            grupo = grupo.sample(n=n_samples_per_class, random_state=42)\n",
    "            \n",
    "            '''train_size = int(train_frac * len(grupo))\n",
    "            test_size = int(test_frac * len(grupo))\n",
    "            \n",
    "            train_df = pd.concat(train_df, grupo[:train_size])\n",
    "            test_df = pd.concat(test_df, grupo[train_size:train_size+test_size])\n",
    "            val_df = pd.concat(val_df, grupo[train_size+test_size:])'''\n",
    "            \n",
    "        train_size = int(train_frac * len(grupo))\n",
    "        test_size = int(test_frac * len(grupo))\n",
    "        \n",
    "        train_df = pd.concat([train_df, grupo[:train_size]])\n",
    "        test_df = pd.concat([test_df, grupo[train_size:train_size+test_size]])\n",
    "        val_df = pd.concat([val_df, grupo[train_size+test_size:]])\n",
    "\n",
    "    # Faz o shuffle nos DataFrames\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    val_df = val_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Converte os DataFrames para Hugging Face Datasets\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    test_ds = Dataset.from_pandas(test_df)\n",
    "    val_ds = Dataset.from_pandas(val_df)\n",
    "    \n",
    "    # Combina num DatasetDict\n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_ds,\n",
    "        'validation': val_ds,\n",
    "        'test': test_ds\n",
    "    })\n",
    "\n",
    "    return dataset_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Loading e Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (15347, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intimacao_conteudo</th>\n",
       "      <th>processo_conteudo</th>\n",
       "      <th>rstREGEX</th>\n",
       "      <th>teorIntimacao</th>\n",
       "      <th>intimacaoPJE</th>\n",
       "      <th>processoPJE</th>\n",
       "      <th>idavisopje</th>\n",
       "      <th>setordestino</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{\"diligencia\": \"intime-se o Município de Nata...</td>\n",
       "      <td>PODER JUDICIÁRIO ESTADO DO RIO GRANDE DO NORTE...</td>\n",
       "      <td>{\"id\": \"18612632\", \"tipoComunicacao\": \"INT\", \"...</td>\n",
       "      <td>{\"Numero\": \"08387198120248205001\", \"Competenci...</td>\n",
       "      <td>18612632</td>\n",
       "      <td>Procuradoria da Saude</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{\"diligencia\": \"intimado acerca da obrigação ...</td>\n",
       "      <td>PODER JUDICIÁRIO DO ESTADO DO RIO GRANDE DO NO...</td>\n",
       "      <td>{\"id\": \"18629258\", \"tipoComunicacao\": \"INT\", \"...</td>\n",
       "      <td>{\"Numero\": \"08464942120228205001\", \"Competenci...</td>\n",
       "      <td>18629258</td>\n",
       "      <td>Procuradoria Administrativa</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{\"diligencia\": \"intime-se a parte exequente p...</td>\n",
       "      <td>PODER JUDICIÁRIO DO ESTADO DO RIO GRANDE DO NO...</td>\n",
       "      <td>{\"id\": \"18629665\", \"tipoComunicacao\": \"INT\", \"...</td>\n",
       "      <td>{\"Numero\": \"01263147320118200001\", \"Competenci...</td>\n",
       "      <td>18629665</td>\n",
       "      <td>APOIO FISCAL</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  intimacao_conteudo processo_conteudo  \\\n",
       "0                                        \n",
       "1                                        \n",
       "3                                        \n",
       "\n",
       "                                            rstREGEX  \\\n",
       "0  [{\"diligencia\": \"intime-se o Município de Nata...   \n",
       "1  [{\"diligencia\": \"intimado acerca da obrigação ...   \n",
       "3  [{\"diligencia\": \"intime-se a parte exequente p...   \n",
       "\n",
       "                                       teorIntimacao  \\\n",
       "0  PODER JUDICIÁRIO ESTADO DO RIO GRANDE DO NORTE...   \n",
       "1  PODER JUDICIÁRIO DO ESTADO DO RIO GRANDE DO NO...   \n",
       "3  PODER JUDICIÁRIO DO ESTADO DO RIO GRANDE DO NO...   \n",
       "\n",
       "                                        intimacaoPJE  \\\n",
       "0  {\"id\": \"18612632\", \"tipoComunicacao\": \"INT\", \"...   \n",
       "1  {\"id\": \"18629258\", \"tipoComunicacao\": \"INT\", \"...   \n",
       "3  {\"id\": \"18629665\", \"tipoComunicacao\": \"INT\", \"...   \n",
       "\n",
       "                                         processoPJE idavisopje  \\\n",
       "0  {\"Numero\": \"08387198120248205001\", \"Competenci...   18612632   \n",
       "1  {\"Numero\": \"08464942120228205001\", \"Competenci...   18629258   \n",
       "3  {\"Numero\": \"01263147320118200001\", \"Competenci...   18629665   \n",
       "\n",
       "                  setordestino Classificacao  labels  \n",
       "0        Procuradoria da Saude                     7  \n",
       "1  Procuradoria Administrativa                     0  \n",
       "3                 APOIO FISCAL                     2  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('./data/pgm-dataset-new.parquet')\n",
    "df.rename(columns={'general_classes': 'labels'}, inplace=True)\n",
    "\n",
    "# Criando uma lista com os nomes das classes\n",
    "classes = np.unique(df['labels'])\n",
    "\n",
    "# Alterando as labels no DataFrame\n",
    "id2label = {i: classe for i, classe in enumerate(classes)}\n",
    "label2id = {classe: i for i, classe in enumerate(classes)}\n",
    "\n",
    "df['labels'] = df['labels'].map(label2id)\n",
    "\n",
    "print(f'Shape: {df.shape}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15347/15347 [01:14<00:00, 205.49it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "df['teorIntimacao_clean'] = df['teorIntimacao'].progress_apply(preprocess_text_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "2    6686\n",
       "0    5401\n",
       "1    1338\n",
       "4    1137\n",
       "7     443\n",
       "5     155\n",
       "6     121\n",
       "3      66\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Split de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['intimacao_conteudo', 'processo_conteudo', 'rstREGEX', 'teorIntimacao', 'intimacaoPJE', 'processoPJE', 'idavisopje', 'setordestino', 'Classificacao', 'labels', 'teorIntimacao_clean'],\n",
       "        num_rows: 1638\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['intimacao_conteudo', 'processo_conteudo', 'rstREGEX', 'teorIntimacao', 'intimacaoPJE', 'processoPJE', 'idavisopje', 'setordestino', 'Classificacao', 'labels', 'teorIntimacao_clean'],\n",
       "        num_rows: 354\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['intimacao_conteudo', 'processo_conteudo', 'rstREGEX', 'teorIntimacao', 'intimacaoPJE', 'processoPJE', 'idavisopje', 'setordestino', 'Classificacao', 'labels', 'teorIntimacao_clean'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = separar_e_dividir_dados(df, 'labels', n_samples_per_class=400, train_frac=0.7, test_frac=0.15, val_frac=0.15)\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Carregando o modelo pré-treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'google-bert/bert-base-multilingual-uncased'\n",
    "\n",
    "# Carrega o tokenizador do modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Carrega o modelo para classificação\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                           num_labels=8,\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Congelando Parâmetros\n",
    "\n",
    "Congelando os parâmetros do modelo base para que todos os 110M de parâmetros não sejam modificados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.base_model.named_parameters():\n",
    "    if not 'pooler' in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1638/1638 [00:00<00:00, 1711.13 examples/s]\n",
      "Map: 100%|██████████| 354/354 [00:00<00:00, 1843.25 examples/s]\n",
      "Map: 100%|██████████| 350/350 [00:00<00:00, 1715.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(texts):\n",
    "    # Retorna o texto tokenizado e truncado\n",
    "    return tokenizer(texts['teorIntimacao_clean'], truncation=True)\n",
    "\n",
    "tokenized_data = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['intimacao_conteudo', 'processo_conteudo', 'rstREGEX', 'teorIntimacao', 'intimacaoPJE', 'processoPJE', 'idavisopje', 'setordestino', 'Classificacao', 'labels', 'teorIntimacao_clean', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1638\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['intimacao_conteudo', 'processo_conteudo', 'rstREGEX', 'teorIntimacao', 'intimacaoPJE', 'processoPJE', 'idavisopje', 'setordestino', 'Classificacao', 'labels', 'teorIntimacao_clean', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 354\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['intimacao_conteudo', 'processo_conteudo', 'rstREGEX', 'teorIntimacao', 'intimacaoPJE', 'processoPJE', 'idavisopje', 'setordestino', 'Classificacao', 'labels', 'teorIntimacao_clean', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 350\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando Data Collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Definindo métricas de avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_metric = evaluate.load('precision')\n",
    "recall_metric = evaluate.load('recall')\n",
    "f1_metric = evaluate.load('f1')\n",
    "accuracy_metric = evaluate.load('accuracy')\n",
    "\n",
    "#clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Get predictions\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Aplicando Softmax para recuperar as probabilidades\n",
    "    predictions = predictions - np.max(predictions)\n",
    "    probabilidades = np.exp(predictions) / np.exp(predictions).sum(-1,\n",
    "                                                                   keepdims=True)\n",
    "    # Predizendo a classe mais provável\n",
    "    predicted_classes = np.argmax(probabilidades, axis=1)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    precision = precision_metric.compute(predictions=predicted_classes, references=labels, average='weighted')[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predicted_classes, references=labels, average='weighted')[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predicted_classes, references=labels, average='weighted')[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predicted_classes, references=labels)[\"accuracy\"]\n",
    "    \n",
    "    return {'precision':precision, 'recall':recall, 'f1-score':f1, 'accuracy':accuracy}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Treinando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EsdrasDaniel\\AppData\\Local\\Temp\\ipykernel_9412\\2266952429.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4100' max='4100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4100/4100 9:02:48, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.002900</td>\n",
       "      <td>1.145136</td>\n",
       "      <td>0.645777</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.546659</td>\n",
       "      <td>0.588571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.979400</td>\n",
       "      <td>1.046957</td>\n",
       "      <td>0.630503</td>\n",
       "      <td>0.611429</td>\n",
       "      <td>0.604815</td>\n",
       "      <td>0.611429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.971100</td>\n",
       "      <td>1.053582</td>\n",
       "      <td>0.662567</td>\n",
       "      <td>0.648571</td>\n",
       "      <td>0.609570</td>\n",
       "      <td>0.648571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.977900</td>\n",
       "      <td>1.048332</td>\n",
       "      <td>0.639359</td>\n",
       "      <td>0.637143</td>\n",
       "      <td>0.607691</td>\n",
       "      <td>0.637143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.958800</td>\n",
       "      <td>1.085661</td>\n",
       "      <td>0.651851</td>\n",
       "      <td>0.637143</td>\n",
       "      <td>0.628577</td>\n",
       "      <td>0.637143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.958300</td>\n",
       "      <td>0.998393</td>\n",
       "      <td>0.635247</td>\n",
       "      <td>0.634286</td>\n",
       "      <td>0.618856</td>\n",
       "      <td>0.634286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.963500</td>\n",
       "      <td>1.055722</td>\n",
       "      <td>0.646686</td>\n",
       "      <td>0.605714</td>\n",
       "      <td>0.587864</td>\n",
       "      <td>0.605714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.060600</td>\n",
       "      <td>1.046773</td>\n",
       "      <td>0.690899</td>\n",
       "      <td>0.637143</td>\n",
       "      <td>0.599284</td>\n",
       "      <td>0.637143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.072900</td>\n",
       "      <td>1.025593</td>\n",
       "      <td>0.670688</td>\n",
       "      <td>0.651429</td>\n",
       "      <td>0.612639</td>\n",
       "      <td>0.651429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.056900</td>\n",
       "      <td>1.024858</td>\n",
       "      <td>0.678450</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.630796</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.046400</td>\n",
       "      <td>1.020460</td>\n",
       "      <td>0.644343</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.626449</td>\n",
       "      <td>0.645714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.057700</td>\n",
       "      <td>1.041926</td>\n",
       "      <td>0.668367</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.603227</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.046100</td>\n",
       "      <td>1.008525</td>\n",
       "      <td>0.655456</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.618864</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.020100</td>\n",
       "      <td>1.008192</td>\n",
       "      <td>0.636412</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.633895</td>\n",
       "      <td>0.657143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.037100</td>\n",
       "      <td>1.001258</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>0.634286</td>\n",
       "      <td>0.610309</td>\n",
       "      <td>0.634286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.066200</td>\n",
       "      <td>1.010781</td>\n",
       "      <td>0.642375</td>\n",
       "      <td>0.637143</td>\n",
       "      <td>0.611569</td>\n",
       "      <td>0.637143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.044600</td>\n",
       "      <td>0.996912</td>\n",
       "      <td>0.644776</td>\n",
       "      <td>0.648571</td>\n",
       "      <td>0.627835</td>\n",
       "      <td>0.648571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.012800</td>\n",
       "      <td>1.001178</td>\n",
       "      <td>0.627211</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.621026</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.039500</td>\n",
       "      <td>1.008015</td>\n",
       "      <td>0.642101</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.623631</td>\n",
       "      <td>0.645714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.026500</td>\n",
       "      <td>1.002375</td>\n",
       "      <td>0.641006</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.621237</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4100, training_loss=1.0199584514338795, metrics={'train_runtime': 32576.6863, 'train_samples_per_second': 1.006, 'train_steps_per_second': 0.126, 'total_flos': 8430628971653952.0, 'train_loss': 1.0199584514338795, 'epoch': 20.0})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 8\n",
    "num_epochs = 20\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/bert-pgm-classification_teacher',\n",
    "    #label_names=classes,\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy='epoch',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
